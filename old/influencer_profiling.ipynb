{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109ff838",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshutil\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m process_single_collaborator\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01municodedata\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "import json\n",
    "import requests\n",
    "from deepface import DeepFace\n",
    "from apify_client import ApifyClient\n",
    "import shutil\n",
    "from openai import OpenAI\n",
    "from utils import process_single_collaborator\n",
    "from datetime import datetime\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "def clean_filename(filename):\n",
    "    # Remove all leading/trailing whitespace\n",
    "    cleaned = filename.strip()\n",
    "    # Normalize Unicode characters (e.g., combining characters)\n",
    "    cleaned = unicodedata.normalize('NFKC', cleaned)\n",
    "    # Remove any non-printable ASCII characters (except common ones like newline if needed, but not for filenames)\n",
    "    # This is a bit aggressive and might remove valid characters, use with caution.\n",
    "    # For filenames, focusing on specific problematic characters is better.\n",
    "    # Or, more simply, filter out control characters:\n",
    "    cleaned = ''.join(c for c in cleaned if c.isprintable()) # or not unicodedata.category(c).startswith('C')\n",
    "    return cleaned # Or keep original case for path if needed\n",
    "\n",
    "\n",
    "def get_apify_limit(apify_key):\n",
    "\n",
    "    url = \"https://api.apify.com/v2/users/me/limits\"\n",
    "\n",
    "    bearer_string = \"Bearer \" + apify_key\n",
    "    payload = {}\n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "        'Authorization': bearer_string\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    data = response.json()\n",
    "\n",
    "    return data[\"data\"][\"current\"][\"monthlyUsageUsd\"]\n",
    "def get_average_statistics(profile_data):\n",
    "    \"\"\"\n",
    "    Function to return average profile statistics\n",
    "    Args:\n",
    "        - profile_data: profile data returned from JSON\n",
    "    \"\"\"\n",
    "    if profile_data is None:\n",
    "        return None, None, None, None\n",
    "    posts_list = profile_data.get(\"latestPosts\",[])\n",
    "    likes_list = []\n",
    "    comments_list = []\n",
    "    views_list = []\n",
    "    engagement_list = []\n",
    "    followers = profile_data.get(\"followersCount\",0)\n",
    "    for post in posts_list:\n",
    "        likes_list.append(post.get(\"likesCount\",0))\n",
    "        comments_list.append(post.get(\"commentsCount\",0))\n",
    "        views_list.append(post.get(\"viewsCount\",0))\n",
    "        if followers >0:\n",
    "            engagement_list.append(100*(post.get(\"likesCount\",0) + post.get(\"commentsCount\",0)) / (followers))\n",
    "    \n",
    "    avg_likes = None\n",
    "    avg_comments = None\n",
    "    avg_views = None\n",
    "    avg_engagement_rate = None\n",
    "    if len(likes_list) > 0:\n",
    "            avg_likes = sum(likes_list) / len(likes_list)\n",
    "    if len(comments_list) > 0:\n",
    "        avg_comments = sum(comments_list) / len(comments_list)\n",
    "    if len(views_list) > 0:\n",
    "        avg_views = sum(views_list) / len(views_list)\n",
    "    if len(engagement_list) > 0:\n",
    "        avg_engagement_rate = sum(engagement_list) / len(engagement_list)\n",
    "    return avg_likes, avg_comments, avg_views, avg_engagement_rate\n",
    "\n",
    "def process_collaborators(output_base_dir):\n",
    "\n",
    "    collaborators_list = [f for f in os.listdir(os.path.join(output_base_dir, \"collaborators\")) \n",
    "                     if not f.startswith('.') and f != '.DS_Store']\n",
    "    \n",
    "    APIFY_KEYS = []\n",
    "    with open(\"apify_tokens.txt\", 'r') as file:\n",
    "        # Read each line and strip any whitespace\n",
    "        APIFY_KEYS = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "    # loop over all the apify keys, find the key that has usage limit more than $1\n",
    "    current_apify_key = None\n",
    "    for key in APIFY_KEYS:\n",
    "        usage = get_apify_limit(key)\n",
    "        if usage <=4:\n",
    "            current_apify_key = key\n",
    "            break\n",
    "    if current_apify_key is None:\n",
    "        print(\"No key with usage limit more than $1 found\")\n",
    "        return\n",
    "        \n",
    "    # Loop over the collaborators folder\n",
    "    for c in collaborators_list:\n",
    "\n",
    "        profile_path = os.path.join(output_base_dir, \"collaborators\", c, \"profile.json\")\n",
    "        photo_path = os.path.join(output_base_dir, \"collaborators\", c, \"profile.jpg\")\n",
    "\n",
    "        print(\"Processing collaborator...\", c)\n",
    "        if os.path.exists(profile_path) and os.path.exists(photo_path):\n",
    "            print(\"Already processed, skipping...\",c)\n",
    "            continue\n",
    "        # if not, then get the JSON from Apify\n",
    "\n",
    "        # Initialize the ApifyClient\n",
    "        client = ApifyClient(current_apify_key)\n",
    "\n",
    "        # Prepare the Actor input\n",
    "        run_input = { \"usernames\": [c] }\n",
    "        # Run the Actor and wait for it to finish\n",
    "        run = client.actor(\"dSCLg0C3YEZ83HzYX\").call(run_input=run_input)\n",
    "\n",
    "        # save json and image\n",
    "        with open(profile_path, 'w') as json_file:\n",
    "            for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "                json.dump(item, json_file, indent=4)\n",
    "                if item.get('profilePicUrl'):\n",
    "                    try:\n",
    "                        response = requests.get(item['profilePicUrl'])\n",
    "                        if response.status_code == 200:\n",
    "                            with open(photo_path, 'wb') as photo_file:\n",
    "                                photo_file.write(response.content)\n",
    "                            print(f\"Profile photo saved for {c}\")\n",
    "                        else:\n",
    "                            print(f\"Failed to download profile photo for {c}: HTTP {response.status_code}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error downloading profile photo for {c}: {str(e)}\")\n",
    "\n",
    "                break  # Added break since we only need first item   \n",
    "\n",
    "def get_deep_face_output(image_path):\n",
    "    \"\"\"\n",
    "    Function to get deep face output. Returns select fields from the following:\n",
    "    f is human, query deep face and obtain these fields:\n",
    "    [{'age': int,\n",
    "    'region': {'x': int,\n",
    "    'y': int,\n",
    "    'w': int,\n",
    "    'h': int,\n",
    "    'left_eye': None,\n",
    "    'right_eye': None},\n",
    "    'face_confidence': float,\n",
    "    'gender': {'Woman': float, 'Man': float},\n",
    "    'dominant_gender': 'Woman',\n",
    "    'race': {'asian': float,\n",
    "    'indian': float,\n",
    "    'black': float,\n",
    "    'white': float,\n",
    "    'middle eastern': float,\n",
    "    'latino hispanic': float},\n",
    "    'dominant_race': 'asian',\n",
    "    'emotion': {'angry': float,\n",
    "    'disgust': float,\n",
    "    'fear': float,\n",
    "    'happy': float,\n",
    "    'sad': float,\n",
    "    'surprise': float,\n",
    "    'neutral': float},\n",
    "    'dominant_emotion': string}]\n",
    "    \n",
    "    Args:\n",
    "        image_path: path to the image\n",
    "    Returns:\n",
    "        age: int\n",
    "        gender: string\n",
    "        dominant_race: string\n",
    "        second_dominant_race: string\n",
    "        dominant_emotion: string\n",
    "    \"\"\"\n",
    "    print(\"Processing deep face on\", image_path)\n",
    "\n",
    "    result = DeepFace.analyze(\n",
    "        img_path = image_path, actions = ['age', 'gender', 'race', 'emotion'], enforce_detection=False, detector_backend = \"retinaface\"\n",
    "    )\n",
    "    if isinstance(result, list):\n",
    "\n",
    "        if result[0]['face_confidence'] < 0.5:\n",
    "            return False, None, None, None, None, None\n",
    "    \n",
    "        age = result[0]['age']\n",
    "        gender = result[0]['dominant_gender']\n",
    "        dominant_race = result[0]['dominant_race']\n",
    "        second_dominant_race = sorted(result[0]['race'].items(), key=lambda x: x[1], reverse=True)[1][0]\n",
    "        dominant_emotion = result[0]['dominant_emotion']\n",
    "        \n",
    "        return True, age, gender, dominant_race, second_dominant_race, dominant_emotion\n",
    "    else:\n",
    "        return False, None, None, None, None, None\n",
    "\n",
    "def create_folders_for_handles(geography):\n",
    "    \"\"\"\n",
    "    Function to loop over a json, fetch username, and if the username folder does not exist in collaborators folder,\n",
    "    create a folder for it.\n",
    "    \"\"\"\n",
    "    # Path to the JSON file and collaborators directory\n",
    "    json_path = \"username-filter.influencer_data.json\"\n",
    "    collaborators_dir = \"./data/{geography}/collaborators\"\n",
    "\n",
    "        \n",
    "    # Read the JSON file\n",
    "    try:\n",
    "        with open(json_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "            # Loop through each username in the JSON\n",
    "            for profile in data:\n",
    "                username = profile[\"username\"]\n",
    "                # Create folder path\n",
    "                folder_path = os.path.join(collaborators_dir, username)\n",
    "                \n",
    "                # Create folder if it doesn't exist\n",
    "                if not os.path.exists(folder_path):\n",
    "                    os.makedirs(folder_path)\n",
    "                    print(f\"Created folder for {username}\")\n",
    "                else:\n",
    "                    print(f\"Folder for {username} already exists\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {json_path} not found\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in {json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def save_profile_details(handle, is_human, age, gender, dominant_race, second_dominant_race, dominant_emotion,followersCount,geography):\n",
    "    if is_human:\n",
    "        human_or_brand = \"human\"\n",
    "        tier = \"influencer\"\n",
    "\n",
    "        if followersCount > 1000:\n",
    "            \n",
    "            if followersCount < 10000:\n",
    "                tier = \"nano\"\n",
    "            elif followersCount < 100000:\n",
    "                tier = \"micro\"\n",
    "            elif followersCount < 500000:\n",
    "                tier = \"macro\"\n",
    "            elif followersCount < 1000000:\n",
    "                tier = \"mega\"\n",
    "            else:\n",
    "                tier = \"celebrity\"\n",
    "        \n",
    "        # check if UGC creator\n",
    "        if \"ugc\" in profile_data.get(\"biography\", \"\").lower():\n",
    "            tier = \"ugc,\" + tier\n",
    "            \n",
    "    else:\n",
    "        tier = \"brand\"\n",
    "        human_or_brand = \"brand\"\n",
    "    tier = \",\".join(set(tier.split(\",\")))\n",
    "    profile_details_ai = {\n",
    "        \"tier\": tier,\n",
    "        \"human_or_brand\": human_or_brand,\n",
    "        \"age\": age,\n",
    "        \"gender\": gender,\n",
    "        \"dominant_race\": dominant_race,\n",
    "        \"second_dominant_race\": second_dominant_race,\n",
    "        \"dominant_emotion\": dominant_emotion\n",
    "    }\n",
    "    # save the json\n",
    "    with open(os.path.join(\"data\",geography,\"collaborators\", handle, \"profile_details.json\"), \"w\") as f:\n",
    "        json.dump(profile_details_ai, f)\n",
    "    \n",
    "    return tier, human_or_brand\n",
    "    \n",
    "def human_or_brand_from_name_bio(handle,geography):\n",
    "    \"\"\"\n",
    "    Function to whether a person is human or not from the bio\n",
    "    \"\"\"\n",
    "    \n",
    "    SUTRA_API_KEY=\"su_huzopoC8sVp556LWODhHpjb0g8K_ArF5q_KSXIKB8Ns0Mjk251UpydDgp9BU\"\n",
    "    url = 'https://api.two.ai/v2'\n",
    "    \n",
    "    client = OpenAI(base_url=url, api_key=SUTRA_API_KEY)\n",
    "    # load the profile data\n",
    "    profile_path = os.path.join(\"data\",geography, \"collaborators\", handle, \"profile.json\")\n",
    "    with open(profile_path, \"r\") as f:\n",
    "        profile_data = json.load(f)\n",
    "    \n",
    "    name = profile_data.get(\"fullName\",\"\")\n",
    "    bio = profile_data.get(\"biography\",\"\")\n",
    "    \n",
    "    prompt =f\"\"\"\n",
    "        You are an expert social media profile analyst, specializing in identifying genuine human accounts and inferring basic demographic information from limited textual data.\n",
    "        Your task is to analyze the provided Instagram profile details to determine if it belongs to a real human, and if so, to infer the most likely gender.\n",
    "\n",
    "        **Profile Data:**\n",
    "        - **Username:** \"{handle}\"\n",
    "        - **Display Name:** \"{name}\"\n",
    "        - **Bio Text:** \"{bio}\"\n",
    "\n",
    "        **Analysis Steps:**\n",
    "        1.  **Human or Not Classification:** Assess the username, display name, and especially the bio text.\n",
    "            * **Look for Human Signals:** Personal interests, specific life roles (e.g., student, artist, professional), expressive language, varied and non-generic descriptions, absence of overt sales pitches or spam.\n",
    "            * **Look for Not Human Signals:** Generic promotional language, cryptocurrency offers, \"DM to earn,\" highly repetitive phrases, suspicious external links not typical for a personal profile, lack of personal context suggesting an automated, spam, or otherwise non-personal account.\n",
    "            * **Prioritize Bio:** The bio is often the strongest indicator.\n",
    "        2.  **Gender Inference (if Human):** If the profile is confidently classified as human, attempt to infer the most likely gender (MALE, FEMALE, or UNDETERMINED).\n",
    "            * **Primary Indicator:** Rely heavily on common gender associations with the Display Name and Username.\n",
    "            * **Secondary Indicator (Subtle):** Very subtle linguistic cues in the bio (use with extreme caution, acknowledge inherent unreliability).\n",
    "            * **Default:** If there is no strong indicator, or if the name is gender-neutral/unknown, default to \"UNDETERMINED\".\n",
    "            * **Crucial Caveat:** Acknowledge that gender inference from text alone is limited, potentially inaccurate, and does not account for all gender identities.\n",
    "        3.  **Format Output:** Compile the findings into the specified JSON object.\n",
    "\n",
    "        **Output Format:**\n",
    "        Your output MUST be a JSON object with the exact keys specified below. Values for each key must strictly adhere to the defined types.\n",
    "        Below is an example of the output format:\n",
    "        ```json\n",
    "        {{\n",
    "            \"is_human\": true,   // boolean: true if human, false if not human\n",
    "            \"gender\": \"UNDETERMINED\" // string: \"MALE\", \"FEMALE\", or \"UNDETERMINED\"\n",
    "        }}\n",
    "        ```\n",
    "\n",
    "        **Guidelines for Each Key:**\n",
    "\n",
    "        * **is_human:**\n",
    "            * `true`: The profile exhibits strong signs of being a genuine human user.\n",
    "            * `false`: The profile exhibits strong signs of being an automated, spam, or otherwise not human account.\n",
    "        * **gender:**\n",
    "            * `\"MALE\"`: Strongly inferred to be male based on common name associations.\n",
    "            * `\"FEMALE\"`: Strongly inferred to be female based on common name associations.\n",
    "            * `\"UNDETERMINED\"`: No strong gender inference possible from the text, name is gender-neutral, or the profile is classified as not human.\n",
    "\n",
    "        Provide only the JSON output. Do NOT include any other conversational text or explanations.\n",
    "        \"\"\"\n",
    "\n",
    "    stream = client.chat.completions.create(model='sutra-v2',\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=1024,\n",
    "            temperature=0,\n",
    "            stream=True\n",
    "    )\n",
    "    is_human = False\n",
    "    gender = None\n",
    "\n",
    "    for chunk in stream:\n",
    "        if len(chunk.choices) > 0:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            \n",
    "            finish_reason = chunk.choices[0].finish_reason\n",
    "            # print(content, finish_reason)\n",
    "            if content and finish_reason is None:\n",
    "                # content from sutra comes in chunks, hence every time query seperately\n",
    "                if \"true\" in content.lower():\n",
    "                    is_human = True\n",
    "                if \"female\" in content.lower():\n",
    "                    gender = \"Woman\"\n",
    "                elif \"male\" in content.lower():\n",
    "                    gender = \"Man\"\n",
    "                \n",
    "\n",
    "    return is_human, gender\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # print(\"Creating profile folders for non-existent ones..\")\n",
    "    # create_folders_for_handles()\n",
    "\n",
    "    # print(\"Scraping data...\")\n",
    "    # process_collaborators(\"./data\")\n",
    "\n",
    "    geography = \"india\"\n",
    "    # geography = \"us_canada\"\n",
    "    folder_path = os.path.join(\"data\",geography,\"collaborators\")\n",
    "\n",
    "    # Define fields for your DataFrame/Excel\n",
    "    fields = [\n",
    "    \"username\",\n",
    "    \"full_name\",\n",
    "    \"biography\",\n",
    "    \"followers_count\",\n",
    "    \"follows_count\",\n",
    "    \"tier\",\n",
    "    \"human_or_brand\",\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"avg_likes\",\n",
    "    \"avg_comments\",\n",
    "    \"avg_views\",\n",
    "    \"avg_engagement_rate\",\n",
    "    ]\n",
    "\n",
    "    # create a data frame\n",
    "    df = pd.DataFrame(columns=fields)\n",
    "    \n",
    "    processed_files = 0\n",
    "    count = 0\n",
    "    handle_list = os.listdir(folder_path)\n",
    "    handle_list = [clean_filename(handle) for handle in handle_list]\n",
    "    handle_list=list(set(handle_list))\n",
    "    handle_list.sort()\n",
    "    total_files = len(handle_list)\n",
    "    added_handles = set()\n",
    "    for creator in handle_list:\n",
    "        print(\"################################################\")\n",
    "        # skip if .DS_Store\n",
    "        creator = creator.strip().lower()\n",
    "        if creator == \".DS_Store\".lower() or creator in added_handles:\n",
    "            continue\n",
    "        processed_files += 1\n",
    "        \n",
    "        added_handles.add(creator)    \n",
    "        print(\"Processing creator number\", processed_files, \"out of total\", total_files)\n",
    "    \n",
    "        # get the full path of the files\n",
    "        profile_details_path = os.path.join(folder_path, creator, \"profile.json\")\n",
    "        profile_details_path_ai = os.path.join(folder_path, creator, \"ai_details.json\")\n",
    "        profile_pic_path = os.path.join(folder_path, creator, \"profile.jpg\")\n",
    "        \n",
    "        if not os.path.exists(profile_pic_path):\n",
    "            # we check only on profile pic - as profile.json gets saved even when incorrect handle\n",
    "            flag = False\n",
    "            print(\"Creator data not present\", creator, \" trying to download...\")\n",
    "            # call apify to download it\n",
    "            process_single_collaborator(creator)\n",
    "            if os.path.exists(profile_pic_path):\n",
    "                flag = True\n",
    "            if not flag:\n",
    "                # delete the creator\n",
    "                print(\"Incorrect handle, deleting it\", creator)\n",
    "                shutil.rmtree(os.path.join(folder_path, creator))\n",
    "\n",
    "                continue\n",
    "\n",
    "\n",
    "        # load the json file\n",
    "        with open(profile_details_path, \"r\") as f:\n",
    "            profile_data = json.load(f)\n",
    "\n",
    "        # get the fields\n",
    "        username = profile_data.get(\"username\",\"\")\n",
    "        full_name = profile_data.get(\"fullName\",\"\")\n",
    "        biography = profile_data.get(\"biography\",\"\")\n",
    "        followers_count = profile_data.get(\"followersCount\",0)\n",
    "        follows_count = profile_data.get(\"followsCount\",0)\n",
    "        \n",
    "        if os.path.exists(profile_details_path_ai):\n",
    "            print(\"Already processed, adding to DF, continue ... \")\n",
    "            # load the info\n",
    "            with open(profile_details_path_ai, \"r\") as f:\n",
    "                profile_details_ai = json.load(f)\n",
    "            tier = profile_details_ai.get(\"tier\", \"\")\n",
    "\n",
    "            # check if UGC creator\n",
    "            ## first check if human, and then check if bio has ugc mentioned\n",
    "            if profile_details_ai.get(\"human_or_brand\", \"\") == \"human\":\n",
    "                if \"ugc\" in profile_data.get(\"biography\", \"\").lower():\n",
    "                    tier = \"ugc,\" + tier\n",
    "            #split by comma, and remove duplicates - NEEDED IN CASE THE CODE IS RERUN\n",
    "            tier = \",\".join(set(tier.split(\",\")))\n",
    "            human_or_brand = profile_details_ai.get(\"human_or_brand\", \"\")\n",
    "            age = profile_details_ai.get(\"age\", None)\n",
    "            gender = profile_details_ai.get(\"gender\", None)\n",
    "\n",
    "            # get average statistics\n",
    "            avg_likes, avg_comments, avg_views, avg_engagement_rate = get_average_statistics(profile_data)\n",
    "\n",
    "            df.loc[len(df)] = [\n",
    "                username,\n",
    "                full_name,\n",
    "                biography,\n",
    "                followers_count,\n",
    "                follows_count,\n",
    "                tier,\n",
    "                human_or_brand,\n",
    "                age,\n",
    "                gender,\n",
    "                avg_likes,\n",
    "                avg_comments,\n",
    "                avg_views,\n",
    "                avg_engagement_rate\n",
    "\n",
    "            ]\n",
    "            \n",
    "            profile_details_ai[\"tier\"] = tier\n",
    "\n",
    "            # save the json\n",
    "            with open(profile_details_path_ai, \"w\") as f:\n",
    "                json.dump(profile_details_ai, f)\n",
    "            continue\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        is_human_namebio = False\n",
    "        is_human_profilepic = False\n",
    "        age = None\n",
    "        gender = None\n",
    "        dominant_race = None\n",
    "        second_dominant_race = None\n",
    "        dominant_emotion = None\n",
    "        avg_likes = None\n",
    "        avg_comments = None\n",
    "        avg_views = None\n",
    "        avg_engagement_rate = None\n",
    "        print(\"Running deepface on profile pic\", creator)\n",
    "\n",
    "        is_human_profilepic, age, gender, dominant_race, second_dominant_race, dominant_emotion = get_deep_face_output(os.path.join(\"./data/\",geography,\"collaborators\", creator, \"profile.jpg\"))\n",
    "        \n",
    "        if not is_human_profilepic:\n",
    "            is_human_namebio, gender = human_or_brand_from_name_bio(username,geography)\n",
    "\n",
    "        tier, human_or_brand = save_profile_details(creator, is_human_profilepic or is_human_namebio, age, gender, dominant_race, second_dominant_race, dominant_emotion,profile_data.get(\"followersCount\",0),geography)\n",
    "\n",
    "        # get average statistics\n",
    "        avg_likes, avg_comments, avg_views, avg_engagement_rate = get_average_statistics(profile_data)\n",
    "        # add the row to the dataframe\n",
    "        df.loc[len(df)] = [\n",
    "            username,\n",
    "            full_name,\n",
    "            biography,\n",
    "            followers_count,\n",
    "            follows_count,\n",
    "            tier,\n",
    "            human_or_brand,\n",
    "            age,\n",
    "            gender,\n",
    "            avg_likes,\n",
    "            avg_comments,\n",
    "            avg_views,\n",
    "            avg_engagement_rate\n",
    "        ]\n",
    "        print(\"################################################\")\n",
    "\n",
    "    # write to excel file\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    for col, field in enumerate(fields):\n",
    "        ws.cell(row=1, column=col+1).value = field\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        for col, val in enumerate(row):\n",
    "            ws.cell(row=index+2, column=col+1).value = val\n",
    "\n",
    "    # while saving, add the date YYYY-MM-DD\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    wb.save(f\"creators_{geography}_{current_time}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe30db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying _.dracarys.x_\n",
      "Classifying _.harshitha_.gowda._\n",
      "Classifying _.heartwants._\n",
      "Classifying _.hett\n",
      "Classifying _.keshavi._\n",
      "Classifying _.kirranchauhan._\n",
      "Classifying _.life_in_pastel._\n",
      "Classifying _.manisha_kulal\n",
      "Classifying _.mile.y\n",
      "Classifying _.minney__06\n",
      "Classifying _.miss_komal_.1026\n",
      "Classifying _.nickyy75_\n",
      "Classifying _.nutrihealth\n",
      "Classifying _.palakvasudeva._\n",
      "Classifying _.phenomenail\n",
      "Classifying _.poojjja._\n",
      "Classifying _.priyanshiii01\n",
      "Classifying _.ranideepa._\n",
      "Classifying _.samridhii_\n",
      "Classifying _.shikhachaudhary._\n",
      "Classifying _.shobhomita\n",
      "Classifying _.shrush_\n",
      "Classifying _.srilakshmi.__\n",
      "Classifying _.sudeshnax.__\n",
      "Classifying _.tanisha._.22._\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object PoolByteStream.__iter__ at 0x7d85c4389540>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yash-sisodia/face-detection/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 406, in __iter__\n",
      "    self.close()\n",
      "  File \"/home/yash-sisodia/face-detection/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 416, in close\n",
      "    with self._pool._optional_thread_lock:\n",
      "  File \"/home/yash-sisodia/face-detection/venv/lib/python3.12/site-packages/httpcore/_synchronization.py\", line 268, in __enter__\n",
      "    self._lock.acquire()\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "\n",
    "GEOGRAPHY = \"india\"\n",
    "BASE_DIR = \"/home/yash-sisodia/face-detection/collaborators\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='https://api.two.ai/v2',\n",
    "    api_key='sutra_gZTZZ2jcFUpLrXFsUfUajWbSUCgYhcrG6ypJuJTv27wKBUcGMwqGxP2Adgyj'\n",
    ")\n",
    "\n",
    "def classify_by_bio(handle, profile_path):\n",
    "    with open(profile_path, \"r\") as f:\n",
    "        profile_data = json.load(f)\n",
    "\n",
    "    name = profile_data.get(\"fullName\", \"\")\n",
    "    bio = profile_data.get(\"biography\", \"\")\n",
    "\n",
    "    prompt = f'''\n",
    "    You are a social media analyst. Given a display name and bio, determine if this is a real human or a brand.\n",
    "\n",
    "    Name: {name}\n",
    "    Bio: {bio}\n",
    "\n",
    "    Only return true (if human) or false (if not human).\n",
    "    '''\n",
    "\n",
    "    stream = client.chat.completions.create(\n",
    "        model='sutra-v2',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=10,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for chunk in stream:\n",
    "        if \"true\" in chunk.choices[0].delta.content.lower():\n",
    "            return True\n",
    "        elif \"false\" in chunk.choices[0].delta.content.lower():\n",
    "            return False\n",
    "\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    handles = [d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d))]\n",
    "    output = {}\n",
    "\n",
    "    creator_handles = [\n",
    "        \"_.dracarys.x_\",\n",
    "        \"_.harshitha_.gowda._\",\n",
    "        \"_.heartwants._\",\n",
    "        \"_.hett\",\n",
    "        \"_.keshavi._\",\n",
    "        \"_.kirranchauhan._\",\n",
    "        \"_.life_in_pastel._\",\n",
    "        \"_.manisha_kulal\",\n",
    "        \"_.mile.y\",\n",
    "        \"_.minney__06\",\n",
    "        \"_.miss_komal_.1026\",\n",
    "        \"_.nickyy75_\",\n",
    "        \"_.nutrihealth\",\n",
    "        \"_.palakvasudeva._\",\n",
    "        \"_.phenomenail\",\n",
    "        \"_.poojjja._\",\n",
    "        \"_.priyanshiii01\",\n",
    "        \"_.ranideepa._\",\n",
    "        \"_.samridhii_\",\n",
    "        \"_.shikhachaudhary._\",\n",
    "        \"_.shobhomita\",\n",
    "        \"_.shrush_\",\n",
    "        \"_.srilakshmi.__\",\n",
    "        \"_.sudeshnax.__\",\n",
    "        \"_.tanisha._.22._\",\n",
    "        \"_.vandana._03\",\n",
    "        \"_3.4ooo\",\n",
    "        \"__.aaddyyyyyyyy\",\n",
    "        \"__.aishi_\",\n",
    "        \"__.ambiverttt\",\n",
    "        \"__.anuraaag\",\n",
    "        \"__.mitara.b.__\",\n",
    "        \"__.prachi.__26\",\n",
    "        \"__.sannss\",\n",
    "        \"__.shammu_zz\",\n",
    "        \"__.soulzy\",\n",
    "        \"__.tashaa.__\",\n",
    "        \"___.ishpreet\",\n",
    "        \"____.jagriti.___\",\n",
    "        \"______jasmine1_______\",\n",
    "        \"_____keppy_____\",\n",
    "        \"_____tequila_girl\",\n",
    "        \"____sanjana___boro____\",\n",
    "        \"____vartikaa___\",\n",
    "        \"___aamu___05\",\n",
    "        \"___antara__ghosh___\",\n",
    "        \"___chinmoy_bora\",\n",
    "        \"___dhanu07___\",\n",
    "        \"___dubey___ji___\",\n",
    "        \"___jasmine_05\",\n",
    "        \"___jelly_bean___\",\n",
    "        \"___kamakshiiiiii\",\n",
    "        \"___malemnganbiii___\",\n",
    "        \"___maxumilian___\",\n",
    "        \"___mikku___\",\n",
    "        \"___nikhat____\",\n",
    "        \"___notyourcupoftea___\",\n",
    "        \"___riyuhhh___\",\n",
    "        \"___sadah____\",\n",
    "        \"___simran__official___\",\n",
    "        \"___srn_9779_\",\n",
    "        \"___tannu20\",\n",
    "        \"___tripathi__\",\n",
    "        \"___twinkling___\",\n",
    "        \"___whatarethose\",\n",
    "        \"__akshitagupta\",\n",
    "        \"__ankitaa._____\",\n",
    "        \"__apurvapawar_\",\n",
    "        \"__ar_yaaa__\",\n",
    "        \"__barbie_grl\",\n",
    "        \"__baruah__96\",\n",
    "        \"__beauty__squad__\",\n",
    "        \"__bellabeautyy\",\n",
    "        \"__bhumi_thakur___\",\n",
    "        \"__bint.abdullah__\",\n",
    "        \"__brownie_love__\",\n",
    "        \"__chachiii_\",\n",
    "        \"__chikka\",\n",
    "        \"__clematiss__\",\n",
    "        \"__darshan____0024\",\n",
    "        \"__devlina__\",\n",
    "        \"__dr__.aastha___.yadav\",\n",
    "        \"__embxx\",\n",
    "        \"__estrellaaaa.__\",\n",
    "        \"__hussainujjain\",\n",
    "        \"__iamleeba._\",\n",
    "        \"__ishannnnn._\",\n",
    "        \"__ishikasachdeva__\",\n",
    "        \"__ishsh__\",\n",
    "        \"__jenny_pereira__\",\n",
    "        \"__jess_mavalia__\",\n",
    "        \"__jessferns__43\",\n",
    "        \"__kajal.bisht__\",\n",
    "        \"__kalitapriyamr__\",\n",
    "        \"__kaur___simran\",\n",
    "        \"__khushiiiii__3\",\n",
    "        \"__kripali\",\n",
    "        \"__lil.jannat\",\n",
    "        \"__lucy.2.0__\",\n",
    "        \"__ma.dh.u___\"\n",
    "    ]\n",
    "\n",
    "    brand_handles = [\n",
    "        \"mynykaa\",\n",
    "        \"nykaanaturals\",\n",
    "        \"nykaaskinrx\",\n",
    "        \"nykaacosmetics\",\n",
    "        \"wowskinscienceindia\",\n",
    "        \"mamaearth.in\",\n",
    "        \"letspurplle\",\n",
    "        \"asabeautyindia\",\n",
    "        \"trysugar\",\n",
    "        \"nathabit.in\",\n",
    "        \"discover.pilgrim\",\n",
    "        \"vlccin\",\n",
    "        \"forestessentials\",\n",
    "        \"faebeautyofficial\",\n",
    "        \"thebodyshopindia\",\n",
    "        \"lovecolorbar\",\n",
    "        \"biotique_world\",\n",
    "        \"maccosmeticsindia\",\n",
    "        \"revlon_india\",\n",
    "        \"bobbibrownindia\",\n",
    "        \"himalaya_facecare\",\n",
    "        \"plumgoodness\",\n",
    "        \"plumbodylovin\",\n",
    "        \"disguisecosmetics\",\n",
    "        \"myglamm\",\n",
    "        \"coloressenceofficial\",\n",
    "        \"rubys.organics\",\n",
    "        \"elle18_india\",\n",
    "        \"livon\",\n",
    "        \"herbalessencesindia\",\n",
    "        \"thebeautycoindia\",\n",
    "        \"loverecode\",\n",
    "        \"facescanada\",\n",
    "        \"clinique_in\",\n",
    "        \"suroskiebeauty\",\n",
    "        \"paccosmetic\",\n",
    "        \"litcosmeticsindia\",\n",
    "        \"beauty_secret_highend_makeup\",\n",
    "        \"biodermaindia\",\n",
    "        \"mymakeupstory_india\",\n",
    "        \"londonprimeindia\",\n",
    "        \"charactercosmeticsindia\",\n",
    "        \"forever52india\",\n",
    "        \"keautybeauty.in\",\n",
    "        \"shopaarel\",\n",
    "        \"justgoldindia\",\n",
    "        \"lashupindia\",\n",
    "        \"saturnhealth.ghc\",\n",
    "        \"skinkraftshop\",\n",
    "        \"marshealth.ghc\",\n",
    "        \"beromtnails\",\n",
    "        \"blissible_cosmetics\",\n",
    "        \"prishebeauty\",\n",
    "        \"kindedbeauty\",\n",
    "        \"abelia.cares\",\n",
    "        \"farmbeautyofficial\",\n",
    "        \"ellement.company\",\n",
    "        \"skinpassword.in\",\n",
    "        \"kaybykatrina\",\n",
    "        \"82e.official\",\n",
    "        \"arata.in\",\n",
    "        \"herbyangelofficial\",\n",
    "        \"nutriglowcosmetics\",\n",
    "        \"oh_ind\",\n",
    "        \"kapiva_official\",\n",
    "        \"mcaffeineofficial\",\n",
    "        \"perforaofficial\",\n",
    "        \"sephora_india\",\n",
    "        \"recodeeverydaymakeup\",\n",
    "        \"officialswissbeauty\",\n",
    "        \"mycosiq\",\n",
    "        \"suganda.co\",\n",
    "        \"myskinq\",\n",
    "        \"auliglow\",\n",
    "        \"theformularx\",\n",
    "        \"himairag\",\n",
    "        \"dermabayskincare\",\n",
    "        \"beardo.official\",\n",
    "        \"themancompany\",\n",
    "        \"reginaldmen.grooming\",\n",
    "        \"bombayshavingcompany\",\n",
    "        \"mybombae__\",\n",
    "        \"yaanman.official\",\n",
    "        \"allman.in\",\n",
    "        \"dollarshaveclub\",\n",
    "        \"oldspice\",\n",
    "        \"oldspiceindia\",\n",
    "        \"oldspicehair\",\n",
    "        \"oldspiceuk\",\n",
    "        \"foxtaleskin\",\n",
    "        \"ohsogaga\",\n",
    "        \"riseformen\",\n",
    "        \"dotandkey.skincare\",\n",
    "        \"gush.beauty\",\n",
    "        \"try.moody\",\n",
    "        \"moxiebeautyofficial\",\n",
    "        \"poshanskincare\",\n",
    "        \"offduty.india\",\n",
    "        \"emprallofficial\",\n",
    "        \"littlebox.india\",\n",
    "        \"tokyo_talkies\",\n",
    "        \"freakinsindia\",\n",
    "        \"newme.asia\",\n",
    "        \"uptownie101\",\n",
    "        \"crayy.heads\",\n",
    "        \"bonkers.corner\",\n",
    "        \"streetstylestoreofficial\",\n",
    "        \"maincharacter_india\",\n",
    "        \"manyavar\",\n",
    "        \"izfworld\",\n",
    "        \"virgio.official\",\n",
    "        \"staywrogn\",\n",
    "        \"theindiangarageco_\",\n",
    "        \"ajiolife\",\n",
    "        \"fabindiaofficial\",\n",
    "        \"bibaindia\",\n",
    "        \"allensollyindia\",\n",
    "        \"wforwoman\",\n",
    "        \"raymond_the_complete_man\",\n",
    "        \"spykarofficial\",\n",
    "        \"siyaramsindia\",\n",
    "        \"kalkifashion\",\n",
    "        \"bunaai\",\n",
    "        \"libasindia\",\n",
    "        \"shopmulmul\",\n",
    "        \"houseofmasaba\",\n",
    "        \"midsummer.india\",\n",
    "        \"endlessummer.shop\",\n",
    "        \"shopaamili\",\n",
    "        \"layrrd\",\n",
    "        \"kicaactive\",\n",
    "        \"fuaarkofficial\",\n",
    "        \"zymratwear\",\n",
    "        \"ovrtrn\",\n",
    "        \"birdeye.india\",\n",
    "        \"hrxbrand\",\n",
    "        \"chapter2drip\"\n",
    "    ]\n",
    "\n",
    "    all_handles = creator_handles + brand_handles\n",
    "\n",
    "    for handle in all_handles:\n",
    "        profile_path = os.path.join(BASE_DIR, handle, \"profile.json\")\n",
    "        if not os.path.exists(profile_path):\n",
    "            print(f\"{handle} profile.json not found.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"Classifying {handle}\")\n",
    "            is_human = classify_by_bio(handle, profile_path)\n",
    "            output[handle] = is_human\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {handle}: {e}\")\n",
    "\n",
    "    date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    with open(f\"{GEOGRAPHY}_bio_classification_{date}.json\", \"w\") as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
