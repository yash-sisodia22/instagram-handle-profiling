{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "352094d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from get_apify_limit import get_apify_limit\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78da10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collaborator... miss_paul27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.instagram-scraper runId:Jxy0ro0rEVDeUaA0m]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.instagram-scraper runId:Jxy0ro0rEVDeUaA0m]\u001b[0m -> 2025-08-08T07:51:51.309Z ACTOR: Pulling Docker image of build YqzcIMK8qHNvsHgL0 from registry.\n",
      "\u001b[36m[apify.instagram-scraper runId:Jxy0ro0rEVDeUaA0m]\u001b[0m -> 2025-08-08T07:51:51.311Z ACTOR: Creating Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:Jxy0ro0rEVDeUaA0m]\u001b[0m -> 2025-08-08T07:51:51.490Z ACTOR: Starting Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:Jxy0ro0rEVDeUaA0m]\u001b[0m -> 2025-08-08T07:51:53.557Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.2\",\"apifyClientVersion\":\"2.12.2\",\"crawleeVersion\":\"3.13.2\",\"osType\":\"Linux\",\"nodeVersion\":\"v20.19.4\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:Jxy0ro0rEVDeUaA0m]\u001b[0m -> 2025-08-08T07:51:53.709Z \u001b[32mINFO\u001b[39m  Results Limit 99999, ACTOR_MAX_PAID_DATASET_ITEMS 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:Jxy0ro0rEVDeUaA0m]\u001b[0m -> 2025-08-08T07:51:53.717Z \u001b[32mINFO\u001b[39m  pushDataMaxAware 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:Jxy0ro0rEVDeUaA0m]\u001b[0m -> 2025-08-08T07:51:53.870Z \u001b[32mINFO\u001b[39m  [Status message]: Starting the scraper with 0 direct URL(s)\n",
      "\u001b[36m[apify.instagram-scraper runId:Jxy0ro0rEVDeUaA0m]\u001b[0m -> 2025-08-08T07:51:54.014Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.instagram-scraper runId:Jxy0ro0rEVDeUaA0m]\u001b[0m -> 2025-08-08T07:51:54.128Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.instagram-scraper runId:Jxy0ro0rEVDeUaA0m]\u001b[0m -> 2025-08-08T07:51:54.205Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":0,\"requestsFailed\":0,\"retryHistogram\":[],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":279}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:Jxy0ro0rEVDeUaA0m]\u001b[0m -> 2025-08-08T07:51:54.208Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Finished! Total 0 requests: 0 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:Jxy0ro0rEVDeUaA0m]\u001b[0m -> Status: RUNNING, Message: Scraper finished\n",
      "\u001b[36m[apify.instagram-scraper runId:Jxy0ro0rEVDeUaA0m]\u001b[0m -> 2025-08-08T07:51:54.227Z \u001b[32mINFO\u001b[39m  [Status message]: Scraper finished\n",
      "\u001b[36m[apify.instagram-scraper runId:Jxy0ro0rEVDeUaA0m]\u001b[0m -> Status: SUCCEEDED, Message: Scraper finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collaborator... daphne.zorah\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.instagram-scraper runId:yyMHD6aXgPrMeUhRT]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.instagram-scraper runId:yyMHD6aXgPrMeUhRT]\u001b[0m -> 2025-08-08T07:52:02.752Z ACTOR: Pulling Docker image of build YqzcIMK8qHNvsHgL0 from registry.\n",
      "\u001b[36m[apify.instagram-scraper runId:yyMHD6aXgPrMeUhRT]\u001b[0m -> 2025-08-08T07:52:02.759Z ACTOR: Creating Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:yyMHD6aXgPrMeUhRT]\u001b[0m -> 2025-08-08T07:52:03.173Z ACTOR: Starting Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:yyMHD6aXgPrMeUhRT]\u001b[0m -> 2025-08-08T07:52:04.824Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.2\",\"apifyClientVersion\":\"2.12.2\",\"crawleeVersion\":\"3.13.2\",\"osType\":\"Linux\",\"nodeVersion\":\"v20.19.4\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:yyMHD6aXgPrMeUhRT]\u001b[0m -> 2025-08-08T07:52:05.082Z \u001b[32mINFO\u001b[39m  Results Limit 99999, ACTOR_MAX_PAID_DATASET_ITEMS 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:yyMHD6aXgPrMeUhRT]\u001b[0m -> 2025-08-08T07:52:05.084Z \u001b[32mINFO\u001b[39m  pushDataMaxAware 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:yyMHD6aXgPrMeUhRT]\u001b[0m -> 2025-08-08T07:52:05.268Z \u001b[32mINFO\u001b[39m  [Status message]: Starting the scraper with 0 direct URL(s)\n",
      "\u001b[36m[apify.instagram-scraper runId:yyMHD6aXgPrMeUhRT]\u001b[0m -> 2025-08-08T07:52:05.434Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.instagram-scraper runId:yyMHD6aXgPrMeUhRT]\u001b[0m -> 2025-08-08T07:52:05.526Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.instagram-scraper runId:yyMHD6aXgPrMeUhRT]\u001b[0m -> 2025-08-08T07:52:05.600Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":0,\"requestsFailed\":0,\"retryHistogram\":[],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":282}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:yyMHD6aXgPrMeUhRT]\u001b[0m -> 2025-08-08T07:52:05.603Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Finished! Total 0 requests: 0 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:yyMHD6aXgPrMeUhRT]\u001b[0m -> Status: RUNNING, Message: Scraper finished\n",
      "\u001b[36m[apify.instagram-scraper runId:yyMHD6aXgPrMeUhRT]\u001b[0m -> 2025-08-08T07:52:05.617Z \u001b[32mINFO\u001b[39m  [Status message]: Scraper finished\n",
      "\u001b[36m[apify.instagram-scraper runId:yyMHD6aXgPrMeUhRT]\u001b[0m -> Status: SUCCEEDED, Message: Scraper finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collaborator... cuddles_by_dr.rainabrar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.instagram-scraper runId:jgBbUbfi6DWaW7IZx]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.instagram-scraper runId:jgBbUbfi6DWaW7IZx]\u001b[0m -> Status: RUNNING, Message: Starting the scraper with 0 direct URL(s)\n",
      "\u001b[36m[apify.instagram-scraper runId:jgBbUbfi6DWaW7IZx]\u001b[0m -> 2025-08-08T07:52:15.211Z ACTOR: Pulling Docker image of build YqzcIMK8qHNvsHgL0 from registry.\n",
      "\u001b[36m[apify.instagram-scraper runId:jgBbUbfi6DWaW7IZx]\u001b[0m -> 2025-08-08T07:52:15.215Z ACTOR: Creating Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:jgBbUbfi6DWaW7IZx]\u001b[0m -> 2025-08-08T07:52:15.361Z ACTOR: Starting Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:jgBbUbfi6DWaW7IZx]\u001b[0m -> 2025-08-08T07:52:16.958Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.2\",\"apifyClientVersion\":\"2.12.2\",\"crawleeVersion\":\"3.13.2\",\"osType\":\"Linux\",\"nodeVersion\":\"v20.19.4\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:jgBbUbfi6DWaW7IZx]\u001b[0m -> 2025-08-08T07:52:17.078Z \u001b[32mINFO\u001b[39m  Results Limit 99999, ACTOR_MAX_PAID_DATASET_ITEMS 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:jgBbUbfi6DWaW7IZx]\u001b[0m -> 2025-08-08T07:52:17.079Z \u001b[32mINFO\u001b[39m  pushDataMaxAware 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:jgBbUbfi6DWaW7IZx]\u001b[0m -> 2025-08-08T07:52:17.225Z \u001b[32mINFO\u001b[39m  [Status message]: Starting the scraper with 0 direct URL(s)\n",
      "\u001b[36m[apify.instagram-scraper runId:jgBbUbfi6DWaW7IZx]\u001b[0m -> 2025-08-08T07:52:17.371Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.instagram-scraper runId:jgBbUbfi6DWaW7IZx]\u001b[0m -> 2025-08-08T07:52:17.495Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.instagram-scraper runId:jgBbUbfi6DWaW7IZx]\u001b[0m -> 2025-08-08T07:52:17.584Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":0,\"requestsFailed\":0,\"retryHistogram\":[],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":284}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:jgBbUbfi6DWaW7IZx]\u001b[0m -> 2025-08-08T07:52:17.591Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Finished! Total 0 requests: 0 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:jgBbUbfi6DWaW7IZx]\u001b[0m -> 2025-08-08T07:52:17.624Z \u001b[32mINFO\u001b[39m  [Status message]: Scraper finished\n",
      "\u001b[36m[apify.instagram-scraper runId:jgBbUbfi6DWaW7IZx]\u001b[0m -> Status: SUCCEEDED, Message: Scraper finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collaborator... girlwithbigdreamz22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.instagram-scraper runId:7vdFxZWbPTdVcx8y0]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.instagram-scraper runId:7vdFxZWbPTdVcx8y0]\u001b[0m -> 2025-08-08T07:52:26.872Z ACTOR: Pulling Docker image of build YqzcIMK8qHNvsHgL0 from registry.\n",
      "\u001b[36m[apify.instagram-scraper runId:7vdFxZWbPTdVcx8y0]\u001b[0m -> 2025-08-08T07:52:26.888Z ACTOR: Creating Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:7vdFxZWbPTdVcx8y0]\u001b[0m -> 2025-08-08T07:52:27.042Z ACTOR: Starting Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:7vdFxZWbPTdVcx8y0]\u001b[0m -> 2025-08-08T07:52:28.765Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.2\",\"apifyClientVersion\":\"2.12.2\",\"crawleeVersion\":\"3.13.2\",\"osType\":\"Linux\",\"nodeVersion\":\"v20.19.4\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:7vdFxZWbPTdVcx8y0]\u001b[0m -> 2025-08-08T07:52:28.913Z \u001b[32mINFO\u001b[39m  Results Limit 99999, ACTOR_MAX_PAID_DATASET_ITEMS 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:7vdFxZWbPTdVcx8y0]\u001b[0m -> 2025-08-08T07:52:28.915Z \u001b[32mINFO\u001b[39m  pushDataMaxAware 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:7vdFxZWbPTdVcx8y0]\u001b[0m -> 2025-08-08T07:52:29.120Z \u001b[32mINFO\u001b[39m  [Status message]: Starting the scraper with 0 direct URL(s)\n",
      "\u001b[36m[apify.instagram-scraper runId:7vdFxZWbPTdVcx8y0]\u001b[0m -> 2025-08-08T07:52:29.244Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.instagram-scraper runId:7vdFxZWbPTdVcx8y0]\u001b[0m -> 2025-08-08T07:52:29.368Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.instagram-scraper runId:7vdFxZWbPTdVcx8y0]\u001b[0m -> 2025-08-08T07:52:29.483Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":0,\"requestsFailed\":0,\"retryHistogram\":[],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":318}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:7vdFxZWbPTdVcx8y0]\u001b[0m -> 2025-08-08T07:52:29.486Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Finished! Total 0 requests: 0 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:7vdFxZWbPTdVcx8y0]\u001b[0m -> 2025-08-08T07:52:29.510Z \u001b[32mINFO\u001b[39m  [Status message]: Scraper finished\n",
      "\u001b[36m[apify.instagram-scraper runId:7vdFxZWbPTdVcx8y0]\u001b[0m -> Status: SUCCEEDED, Message: Scraper finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collaborator... fitindiaoff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.instagram-scraper runId:IQ8lnvdldXNrhXj94]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.instagram-scraper runId:IQ8lnvdldXNrhXj94]\u001b[0m -> 2025-08-08T07:52:38.451Z ACTOR: Pulling Docker image of build YqzcIMK8qHNvsHgL0 from registry.\n",
      "\u001b[36m[apify.instagram-scraper runId:IQ8lnvdldXNrhXj94]\u001b[0m -> 2025-08-08T07:52:38.454Z ACTOR: Creating Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:IQ8lnvdldXNrhXj94]\u001b[0m -> 2025-08-08T07:52:38.642Z ACTOR: Starting Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:IQ8lnvdldXNrhXj94]\u001b[0m -> 2025-08-08T07:52:40.322Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.2\",\"apifyClientVersion\":\"2.12.2\",\"crawleeVersion\":\"3.13.2\",\"osType\":\"Linux\",\"nodeVersion\":\"v20.19.4\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:IQ8lnvdldXNrhXj94]\u001b[0m -> 2025-08-08T07:52:40.429Z \u001b[32mINFO\u001b[39m  Results Limit 99999, ACTOR_MAX_PAID_DATASET_ITEMS 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:IQ8lnvdldXNrhXj94]\u001b[0m -> 2025-08-08T07:52:40.431Z \u001b[32mINFO\u001b[39m  pushDataMaxAware 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:IQ8lnvdldXNrhXj94]\u001b[0m -> 2025-08-08T07:52:40.573Z \u001b[32mINFO\u001b[39m  [Status message]: Starting the scraper with 0 direct URL(s)\n",
      "\u001b[36m[apify.instagram-scraper runId:IQ8lnvdldXNrhXj94]\u001b[0m -> 2025-08-08T07:52:40.718Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.instagram-scraper runId:IQ8lnvdldXNrhXj94]\u001b[0m -> 2025-08-08T07:52:40.807Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.instagram-scraper runId:IQ8lnvdldXNrhXj94]\u001b[0m -> 2025-08-08T07:52:40.874Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":0,\"requestsFailed\":0,\"retryHistogram\":[],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":249}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:IQ8lnvdldXNrhXj94]\u001b[0m -> 2025-08-08T07:52:40.876Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Finished! Total 0 requests: 0 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:IQ8lnvdldXNrhXj94]\u001b[0m -> 2025-08-08T07:52:40.891Z \u001b[32mINFO\u001b[39m  [Status message]: Scraper finished\n",
      "\u001b[36m[apify.instagram-scraper runId:IQ8lnvdldXNrhXj94]\u001b[0m -> Status: SUCCEEDED, Message: Scraper finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collaborator... raddtrip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.instagram-scraper runId:Boy9cqcc3SJekaZiN]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.instagram-scraper runId:Boy9cqcc3SJekaZiN]\u001b[0m -> 2025-08-08T07:52:49.997Z ACTOR: Pulling Docker image of build YqzcIMK8qHNvsHgL0 from registry.\n",
      "\u001b[36m[apify.instagram-scraper runId:Boy9cqcc3SJekaZiN]\u001b[0m -> 2025-08-08T07:52:50.000Z ACTOR: Creating Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:Boy9cqcc3SJekaZiN]\u001b[0m -> 2025-08-08T07:52:50.110Z ACTOR: Starting Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:Boy9cqcc3SJekaZiN]\u001b[0m -> 2025-08-08T07:52:52.242Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.2\",\"apifyClientVersion\":\"2.12.2\",\"crawleeVersion\":\"3.13.2\",\"osType\":\"Linux\",\"nodeVersion\":\"v20.19.4\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:Boy9cqcc3SJekaZiN]\u001b[0m -> 2025-08-08T07:52:52.414Z \u001b[32mINFO\u001b[39m  Results Limit 99999, ACTOR_MAX_PAID_DATASET_ITEMS 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:Boy9cqcc3SJekaZiN]\u001b[0m -> 2025-08-08T07:52:52.414Z \u001b[32mINFO\u001b[39m  pushDataMaxAware 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:Boy9cqcc3SJekaZiN]\u001b[0m -> 2025-08-08T07:52:52.541Z \u001b[32mINFO\u001b[39m  [Status message]: Starting the scraper with 0 direct URL(s)\n",
      "\u001b[36m[apify.instagram-scraper runId:Boy9cqcc3SJekaZiN]\u001b[0m -> 2025-08-08T07:52:52.682Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.instagram-scraper runId:Boy9cqcc3SJekaZiN]\u001b[0m -> 2025-08-08T07:52:52.764Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.instagram-scraper runId:Boy9cqcc3SJekaZiN]\u001b[0m -> 2025-08-08T07:52:52.846Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":0,\"requestsFailed\":0,\"retryHistogram\":[],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":233}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:Boy9cqcc3SJekaZiN]\u001b[0m -> 2025-08-08T07:52:52.849Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Finished! Total 0 requests: 0 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:Boy9cqcc3SJekaZiN]\u001b[0m -> 2025-08-08T07:52:52.862Z \u001b[32mINFO\u001b[39m  [Status message]: Scraper finished\n",
      "\u001b[36m[apify.instagram-scraper runId:Boy9cqcc3SJekaZiN]\u001b[0m -> Status: SUCCEEDED, Message: Scraper finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collaborator... laddu_gopal_outfit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.instagram-scraper runId:QdFQ6yczXTT7hlWIO]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.instagram-scraper runId:QdFQ6yczXTT7hlWIO]\u001b[0m -> Status: RUNNING, Message: Starting the scraper with 0 direct URL(s)\n",
      "\u001b[36m[apify.instagram-scraper runId:QdFQ6yczXTT7hlWIO]\u001b[0m -> 2025-08-08T07:53:02.081Z ACTOR: Pulling Docker image of build YqzcIMK8qHNvsHgL0 from registry.\n",
      "\u001b[36m[apify.instagram-scraper runId:QdFQ6yczXTT7hlWIO]\u001b[0m -> 2025-08-08T07:53:02.083Z ACTOR: Creating Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:QdFQ6yczXTT7hlWIO]\u001b[0m -> 2025-08-08T07:53:02.290Z ACTOR: Starting Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:QdFQ6yczXTT7hlWIO]\u001b[0m -> 2025-08-08T07:53:03.818Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.2\",\"apifyClientVersion\":\"2.12.2\",\"crawleeVersion\":\"3.13.2\",\"osType\":\"Linux\",\"nodeVersion\":\"v20.19.4\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:QdFQ6yczXTT7hlWIO]\u001b[0m -> 2025-08-08T07:53:03.936Z \u001b[32mINFO\u001b[39m  Results Limit 99999, ACTOR_MAX_PAID_DATASET_ITEMS 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:QdFQ6yczXTT7hlWIO]\u001b[0m -> 2025-08-08T07:53:03.938Z \u001b[32mINFO\u001b[39m  pushDataMaxAware 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:QdFQ6yczXTT7hlWIO]\u001b[0m -> 2025-08-08T07:53:04.139Z \u001b[32mINFO\u001b[39m  [Status message]: Starting the scraper with 0 direct URL(s)\n",
      "\u001b[36m[apify.instagram-scraper runId:QdFQ6yczXTT7hlWIO]\u001b[0m -> 2025-08-08T07:53:04.277Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.instagram-scraper runId:QdFQ6yczXTT7hlWIO]\u001b[0m -> 2025-08-08T07:53:04.447Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.instagram-scraper runId:QdFQ6yczXTT7hlWIO]\u001b[0m -> 2025-08-08T07:53:04.597Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":0,\"requestsFailed\":0,\"retryHistogram\":[],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":401}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:QdFQ6yczXTT7hlWIO]\u001b[0m -> 2025-08-08T07:53:04.603Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Finished! Total 0 requests: 0 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:QdFQ6yczXTT7hlWIO]\u001b[0m -> 2025-08-08T07:53:04.615Z \u001b[32mINFO\u001b[39m  [Status message]: Scraper finished\n",
      "\u001b[36m[apify.instagram-scraper runId:QdFQ6yczXTT7hlWIO]\u001b[0m -> Status: SUCCEEDED, Message: Scraper finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collaborator... metamonsk.in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.instagram-scraper runId:vd53NbQFSMaCnaut6]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.instagram-scraper runId:vd53NbQFSMaCnaut6]\u001b[0m -> Status: RUNNING, Message: Scraper finished\n",
      "\u001b[36m[apify.instagram-scraper runId:vd53NbQFSMaCnaut6]\u001b[0m -> 2025-08-08T07:53:13.581Z ACTOR: Pulling Docker image of build YqzcIMK8qHNvsHgL0 from registry.\n",
      "\u001b[36m[apify.instagram-scraper runId:vd53NbQFSMaCnaut6]\u001b[0m -> 2025-08-08T07:53:13.582Z ACTOR: Creating Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:vd53NbQFSMaCnaut6]\u001b[0m -> 2025-08-08T07:53:13.713Z ACTOR: Starting Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:vd53NbQFSMaCnaut6]\u001b[0m -> 2025-08-08T07:53:15.133Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.2\",\"apifyClientVersion\":\"2.12.2\",\"crawleeVersion\":\"3.13.2\",\"osType\":\"Linux\",\"nodeVersion\":\"v20.19.4\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:vd53NbQFSMaCnaut6]\u001b[0m -> 2025-08-08T07:53:15.229Z \u001b[32mINFO\u001b[39m  Results Limit 99999, ACTOR_MAX_PAID_DATASET_ITEMS 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:vd53NbQFSMaCnaut6]\u001b[0m -> 2025-08-08T07:53:15.230Z \u001b[32mINFO\u001b[39m  pushDataMaxAware 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:vd53NbQFSMaCnaut6]\u001b[0m -> 2025-08-08T07:53:15.346Z \u001b[32mINFO\u001b[39m  [Status message]: Starting the scraper with 0 direct URL(s)\n",
      "\u001b[36m[apify.instagram-scraper runId:vd53NbQFSMaCnaut6]\u001b[0m -> 2025-08-08T07:53:15.444Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.instagram-scraper runId:vd53NbQFSMaCnaut6]\u001b[0m -> 2025-08-08T07:53:15.523Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.instagram-scraper runId:vd53NbQFSMaCnaut6]\u001b[0m -> 2025-08-08T07:53:15.641Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":0,\"requestsFailed\":0,\"retryHistogram\":[],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":259}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:vd53NbQFSMaCnaut6]\u001b[0m -> 2025-08-08T07:53:15.642Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Finished! Total 0 requests: 0 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:vd53NbQFSMaCnaut6]\u001b[0m -> 2025-08-08T07:53:15.665Z \u001b[32mINFO\u001b[39m  [Status message]: Scraper finished\n",
      "\u001b[36m[apify.instagram-scraper runId:vd53NbQFSMaCnaut6]\u001b[0m -> Status: SUCCEEDED, Message: Scraper finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collaborator... naazukjewels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.instagram-scraper runId:D3MZ6sZ0Hp74dBjxj]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.instagram-scraper runId:D3MZ6sZ0Hp74dBjxj]\u001b[0m -> Status: RUNNING, Message: Starting the scraper with 0 direct URL(s)\n",
      "\u001b[36m[apify.instagram-scraper runId:D3MZ6sZ0Hp74dBjxj]\u001b[0m -> 2025-08-08T07:53:25.332Z ACTOR: Pulling Docker image of build YqzcIMK8qHNvsHgL0 from registry.\n",
      "\u001b[36m[apify.instagram-scraper runId:D3MZ6sZ0Hp74dBjxj]\u001b[0m -> 2025-08-08T07:53:25.334Z ACTOR: Creating Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:D3MZ6sZ0Hp74dBjxj]\u001b[0m -> 2025-08-08T07:53:25.445Z ACTOR: Starting Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:D3MZ6sZ0Hp74dBjxj]\u001b[0m -> 2025-08-08T07:53:27.087Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.2\",\"apifyClientVersion\":\"2.12.2\",\"crawleeVersion\":\"3.13.2\",\"osType\":\"Linux\",\"nodeVersion\":\"v20.19.4\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:D3MZ6sZ0Hp74dBjxj]\u001b[0m -> 2025-08-08T07:53:27.235Z \u001b[32mINFO\u001b[39m  Results Limit 99999, ACTOR_MAX_PAID_DATASET_ITEMS 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:D3MZ6sZ0Hp74dBjxj]\u001b[0m -> 2025-08-08T07:53:27.237Z \u001b[32mINFO\u001b[39m  pushDataMaxAware 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:D3MZ6sZ0Hp74dBjxj]\u001b[0m -> 2025-08-08T07:53:27.389Z \u001b[32mINFO\u001b[39m  [Status message]: Starting the scraper with 0 direct URL(s)\n",
      "\u001b[36m[apify.instagram-scraper runId:D3MZ6sZ0Hp74dBjxj]\u001b[0m -> 2025-08-08T07:53:27.512Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.instagram-scraper runId:D3MZ6sZ0Hp74dBjxj]\u001b[0m -> 2025-08-08T07:53:27.608Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.instagram-scraper runId:D3MZ6sZ0Hp74dBjxj]\u001b[0m -> 2025-08-08T07:53:27.768Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":0,\"requestsFailed\":0,\"retryHistogram\":[],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":345}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:D3MZ6sZ0Hp74dBjxj]\u001b[0m -> 2025-08-08T07:53:27.770Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Finished! Total 0 requests: 0 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:D3MZ6sZ0Hp74dBjxj]\u001b[0m -> 2025-08-08T07:53:27.804Z \u001b[32mINFO\u001b[39m  [Status message]: Scraper finished\n",
      "\u001b[36m[apify.instagram-scraper runId:D3MZ6sZ0Hp74dBjxj]\u001b[0m -> Status: SUCCEEDED, Message: Scraper finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collaborator... priyankadebnath73884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.instagram-scraper runId:ovGMLKPnJQdSgEmPK]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.instagram-scraper runId:ovGMLKPnJQdSgEmPK]\u001b[0m -> 2025-08-08T07:53:36.614Z ACTOR: Pulling Docker image of build YqzcIMK8qHNvsHgL0 from registry.\n",
      "\u001b[36m[apify.instagram-scraper runId:ovGMLKPnJQdSgEmPK]\u001b[0m -> 2025-08-08T07:53:36.616Z ACTOR: Creating Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:ovGMLKPnJQdSgEmPK]\u001b[0m -> 2025-08-08T07:53:36.719Z ACTOR: Starting Docker container.\n",
      "\u001b[36m[apify.instagram-scraper runId:ovGMLKPnJQdSgEmPK]\u001b[0m -> 2025-08-08T07:53:38.347Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.2\",\"apifyClientVersion\":\"2.12.2\",\"crawleeVersion\":\"3.13.2\",\"osType\":\"Linux\",\"nodeVersion\":\"v20.19.4\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:ovGMLKPnJQdSgEmPK]\u001b[0m -> 2025-08-08T07:53:38.494Z \u001b[32mINFO\u001b[39m  Results Limit 99999, ACTOR_MAX_PAID_DATASET_ITEMS 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:ovGMLKPnJQdSgEmPK]\u001b[0m -> 2025-08-08T07:53:38.497Z \u001b[32mINFO\u001b[39m  pushDataMaxAware 1761\n",
      "\u001b[36m[apify.instagram-scraper runId:ovGMLKPnJQdSgEmPK]\u001b[0m -> 2025-08-08T07:53:38.662Z \u001b[32mINFO\u001b[39m  [Status message]: Starting the scraper with 0 direct URL(s)\n",
      "\u001b[36m[apify.instagram-scraper runId:ovGMLKPnJQdSgEmPK]\u001b[0m -> 2025-08-08T07:53:38.806Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.instagram-scraper runId:ovGMLKPnJQdSgEmPK]\u001b[0m -> 2025-08-08T07:53:38.882Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.instagram-scraper runId:ovGMLKPnJQdSgEmPK]\u001b[0m -> 2025-08-08T07:53:38.992Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":0,\"requestsFailed\":0,\"retryHistogram\":[],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":295}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:ovGMLKPnJQdSgEmPK]\u001b[0m -> 2025-08-08T07:53:39.000Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Finished! Total 0 requests: 0 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:ovGMLKPnJQdSgEmPK]\u001b[0m -> Status: RUNNING, Message: Scraper finished\n",
      "\u001b[36m[apify.instagram-scraper runId:ovGMLKPnJQdSgEmPK]\u001b[0m -> 2025-08-08T07:53:39.029Z \u001b[32mINFO\u001b[39m  [Status message]: Scraper finished\n",
      "\u001b[36m[apify.instagram-scraper runId:ovGMLKPnJQdSgEmPK]\u001b[0m -> Status: SUCCEEDED, Message: Scraper finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All collaborators processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from apify_client import ApifyClient\n",
    "\n",
    "def process_collaborators(output_base_dir):\n",
    "\n",
    "    collaborators_list = [f for f in os.listdir(os.path.join(output_base_dir, \"collaborators_india/collaborators\")) if not f.startswith('.') and f != '.DS_Store']\n",
    "    \n",
    "    APIFY_KEYS = []\n",
    "    with open(\"apify_api_keys_yash.txt\", 'r') as f:\n",
    "        APIFY_KEYS = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    current_apify_key = None\n",
    "    for key in APIFY_KEYS:\n",
    "        usage = get_apify_limit(key)\n",
    "        if usage <= 4:\n",
    "            current_apify_key = key\n",
    "            break\n",
    "    if current_apify_key is None:\n",
    "        print(\"No key with usage limit more than $1 found\")\n",
    "        return\n",
    "\n",
    "    for c in collaborators_list[:2]:\n",
    "\n",
    "        profile_dir = os.path.join(output_base_dir, \"collaborators_india\", c)\n",
    "        profile_path = os.path.join(profile_dir, \"profile.json\")\n",
    "        photo_path = os.path.join(profile_dir, \"profile.jpg\")\n",
    "\n",
    "        print(\"Processing collaborator...\", c)\n",
    "        \n",
    "        if os.path.exists(profile_path) and os.path.exists(photo_path):\n",
    "            print(f\"Profile already exists for {c}\")\n",
    "        else:\n",
    "            client = ApifyClient(current_apify_key)\n",
    "            run_input = { \"usernames\": [c] }\n",
    "            run = client.actor(\"apify/instagram-scraper\").call(run_input=run_input)\n",
    "\n",
    "            os.makedirs(profile_dir, exist_ok=True)\n",
    "            for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "                with open(profile_path, 'w') as json_file:\n",
    "                    json.dump(item, json_file, indent=4)\n",
    "\n",
    "                if item.get('profilePicUrl'):\n",
    "                    try:\n",
    "                        response = requests.get(item['profilePicUrl'])\n",
    "                        if response.status_code == 200:\n",
    "                            with open(photo_path, 'wb') as photo_file:\n",
    "                                photo_file.write(response.content)\n",
    "                            print(f\"Profile photo saved for {c}\")\n",
    "                        else:\n",
    "                            print(f\"Failed to download profile photo for {c}: HTTP {response.status_code}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error downloading profile photo for {c}: {str(e)}\")\n",
    "                break\n",
    "        \n",
    "        # Processing latestIgtvVideos & latestPosts -------------\n",
    "\n",
    "        with open(profile_path, 'r') as json_file:\n",
    "            profile_data = json.load(json_file)\n",
    "        \n",
    "        content_types = {\n",
    "            \"latestIgtvVideos\": os.path.join(profile_dir, \"latestIgtvVideos\"),\n",
    "            \"latestPosts\": os.path.join(profile_dir, \"latestPosts\")\n",
    "        }\n",
    "\n",
    "        for content_type, content_folder in content_types.items():\n",
    "            items_list = profile_data.get(content_type, [])\n",
    "            os.makedirs(content_folder, exist_ok=True)\n",
    "\n",
    "            for item in items_list:\n",
    "                display_url = item.get(\"displayUrl\")\n",
    "                shortcode = item.get(\"shortCode\")\n",
    "\n",
    "                if display_url and shortcode:\n",
    "                    image_filename = f\"thumbnail_{shortcode}.jpg\"\n",
    "                    image_path = os.path.join(content_folder, image_filename)\n",
    "\n",
    "                    if os.path.exists(image_path):\n",
    "                        print(f\"Thumbnail already exists: {image_filename}\")\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        response = requests.get(display_url)\n",
    "                        if response.status_code == 200:\n",
    "                            with open(image_path, 'wb') as img_file:\n",
    "                                img_file.write(response.content)\n",
    "                            print(f\"Saved thumbnail: {image_filename}\")\n",
    "                        else:\n",
    "                            print(f\"Failed to download {image_filename}: HTTP {response.status_code}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error downloading {image_filename}: {str(e)}\")\n",
    "\n",
    "process_collaborators(\"/home/yash-sisodia/face-detection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
